{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'base (Python 3.11.4)'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import LSTM, Dense, BatchNormalization, Dropout, LayerNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU, Input, RepeatVector,TimeDistributed\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'base (Python 3.11.4)'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'base (Python 3.11.4)'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def generate_lstm_multi_step(X_train, y_train):\n",
    "    \n",
    "    n_timesteps, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "\n",
    "    model = Sequential()\n",
    "    # First LSTM layer with LayerNormalization and recurrent dropout\n",
    "    model.add(LSTM(256, input_shape=(n_timesteps, n_features),\n",
    "                return_sequences=True,\n",
    "                activation='tanh', recurrent_activation='sigmoid',\n",
    "                recurrent_dropout=0.2,\n",
    "                kernel_regularizer=l2(0.001)))\n",
    "    model.add(LSTM(256,\n",
    "                activation='tanh', recurrent_activation='sigmoid',\n",
    "                recurrent_dropout=0.2,\n",
    "                kernel_regularizer=l2(0.001)))\n",
    "    model.add(RepeatVector(n_outputs))\n",
    "    model.add(LSTM(128, activation='tanh', recurrent_activation='sigmoid',\n",
    "                return_sequences=True, recurrent_dropout=0.2,\n",
    "                kernel_regularizer=l2(0.001)))\n",
    "    model.add(LSTM(128, activation='tanh', recurrent_activation='sigmoid',\n",
    "                return_sequences=True, recurrent_dropout=0.2,\n",
    "                kernel_regularizer=l2(0.001)))\n",
    "\n",
    "    model.add(TimeDistributed(Dense(64, activation='tanh')))\n",
    "    model.add(TimeDistributed(Dense(n_features)))\n",
    "    \n",
    "    # Compile the model using AdamW optimizer and a learning rate scheduler\n",
    "    optimizer = AdamW(learning_rate=0.001, weight_decay=1e-5)  # AdamW improves generalization\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'base (Python 3.11.4)'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def train_lstm_multi_step(model, checkpoint_path, X_train, y_train,\n",
    "                          epochs=100, batch_size = 100,\n",
    "                          validation_split=0.05,\n",
    "                          patience = 100,\n",
    "                          verbose = 0):\n",
    "    try:\n",
    "        model.load_weights(checkpoint_path) \n",
    "        # with open(os.path.join(checkpoint_path.split('/')[1],checkpoint_path.split('/')[-1].split('.')[0]), 'rb') as f:\n",
    "        #     history = pickle.load(f)\n",
    "    except:\n",
    "        def lr_scheduler(epoch, lr):\n",
    "        # if epoch < 10:\n",
    "            return lr\n",
    "        # Learning rate scheduler callback\n",
    "        lr_scheduler_callback = LearningRateScheduler(lr_scheduler)\n",
    "\n",
    "        checkpoint = ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                    monitor='val_loss', \n",
    "                                    save_best_only=True,\n",
    "                                    mode='min',  \n",
    "                                    verbose=verbose)\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True, verbose=1)\n",
    "\n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, \n",
    "                            validation_split=validation_split,\n",
    "                            verbose=verbose, callbacks=[checkpoint, lr_scheduler_callback, early_stopping])# ,early_stopping]) \n",
    "        \n",
    "        with open(os.path.join(checkpoint_path.split('/')[1],checkpoint_path.split('/')[-1].split('.')[0]), 'wb') as f:\n",
    "            pickle.dump(history, f)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m'base (Python 3.11.4)'(으)로 셀을 실행하려면 ipykernel 패키지가 필요합니다.\n",
      "\u001b[1;31m다음 명령어를 실행하여 Python 환경에 'ipykernel'을(를) 설치합니다. \n",
      "\u001b[1;31m 명령: 'conda install -n base ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, n_features, n_outputs):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=n_features, hidden_size=512, num_layers=1, batch_first=True, dropout=0.5)\n",
    "        self.dropout1 = nn.Dropout(0.2)  # Add a dropout layer after LSTM\n",
    "        self.lstm2 = nn.LSTM(input_size=512, hidden_size=512, num_layers=1, batch_first=True, dropout=0.5)\n",
    "        self.dropout2 = nn.Dropout(0.2)  # Add a dropout layer after LSTM\n",
    "        self.repeat = n_outputs\n",
    "        self.lstm3 = nn.LSTM(input_size=512, hidden_size=512, num_layers=1, batch_first=True, dropout=0.5)\n",
    "        self.dropout3 = nn.Dropout(0.2)  # Add a dropout layer after LSTM\n",
    "        self.lstm4 = nn.LSTM(input_size=512, hidden_size=512, num_layers=1, batch_first=True, dropout=0.5)\n",
    "        self.dropout4 = nn.Dropout(0.2)  # Add a dropout layer after LSTM\n",
    "        self.dense1 = nn.Linear(512, 256)\n",
    "        self.dropout5 = nn.Dropout(0.2)  # Add a dropout layer after LSTM\n",
    "        self.dense2 = nn.Linear(256, n_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = x[:, -1, :].unsqueeze(1).repeat(1, self.repeat, 1)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = torch.tanh(self.dense1(x))\n",
    "        x = self.dropout5(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "def generate_lstm_multi_step(X_train, y_train):\n",
    "    _, n_features, n_outputs = X_train.shape[1], X_train.shape[2], y_train.shape[1]\n",
    "    model = LSTMModel(n_features, n_outputs)\n",
    "    model.to(device)  # Move the model to the GPU\n",
    "    return model\n",
    "\n",
    "def train_lstm_multi_step(model, checkpoint_path, X_train, y_train, epochs=100, batch_size=100, validation_split=0.05, patience=10, verbose=0):\n",
    "    # Creating training and validation datasets\n",
    "    train_size = int((1 - validation_split) * X_train.size(0))\n",
    "    train_dataset = TensorDataset(X_train[:train_size], y_train[:train_size])\n",
    "    val_dataset = TensorDataset(X_train[train_size:], y_train[train_size:])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "    criterion = nn.MSELoss()\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda=lambda epoch: 1)  # Adjust lambda function as needed\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    history = {'train_loss':[],'val_loss':[]}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                output = model(X_batch)\n",
    "                val_loss += criterion(output, y_batch).item()        \n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        train_loss /= len(train_loader)\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss / len(train_loader)}, Val Loss: {val_loss}\")\n",
    "            \n",
    "        if val_loss < best_loss:\n",
    "            best_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    with open(f\"{os.path.splitext(checkpoint_path)[0]}.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\"val_loss\": best_loss}, f)\n",
    "    return history\n",
    "\n",
    "# Example usage:\n",
    "# X_train, y_train are assumed to be preprocessed and loaded as torch tensors\n",
    "# checkpoint_path = \"./checkpoint.pth\"\n",
    "# model = generate_lstm_multi_step(X_train, y_train)\n",
    "# history = train_lstm_multi_step(model, checkpoint_path, X_train, y_train, batch_size=128, epochs=100, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
